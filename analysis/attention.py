# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AVda_iwJTNZdOQYAcQvIPjAXOF21gjSh
"""

"""
analysis/attention.py - Complete Attention Analysis Suite

All metrics for testing hypothesis:
"Fine-tuning shapes long-range attention dependencies"

Includes:
1. Attention rollout (residual-aware)
2. Dependency distance (long-range quantification)
3. Spatial coherence (Moran's I)
4. Attention entropy (focus measure)

All functions are layer-wise, head-wise, and CLS-aware.
"""

import torch
import numpy as np
from typing import List, Dict, Tuple
from scipy.spatial.distance import cdist


# ============================================================================
# ATTENTION ROLLOUT
# ============================================================================

def compute_attention_rollout(
    attention_maps: List[torch.Tensor],
    discard_ratio: float = 0.0,
    include_residual: bool = True
) -> torch.Tensor:
    """
    Compute attention rollout across layers.

    What rollout DOES:
    - Traces cumulative attention flow from input patches to output
    - Accounts for residual connections in ViT

    What rollout DOES NOT:
    - Prove causal relationships
    - Show reasoning chains (correlation ≠ causation)

    Mathematical basis:
    With residuals: x_out = x_in + Attn(x_in)
    Rollout: A_l = A_l @ (0.5 * A_{l-1} + 0.5 * I)

    Args:
        attention_maps: List of [B, num_heads, N, N] tensors, one per layer
        discard_ratio: Fraction of lowest attention to zero out
        include_residual: Whether to account for residual connections

    Returns:
        rollout: [B, N, N] - cumulative attention from input to final layer
    """
    if not attention_maps:
        raise ValueError("Empty attention_maps")

    B, H, N, _ = attention_maps[0].shape
    device = attention_maps[0].device

    # Average over heads per layer
    attention_layers = [attn.mean(dim=1) for attn in attention_maps]  # List of [B, N, N]

    # Apply discarding if requested
    if discard_ratio > 0:
        for i in range(len(attention_layers)):
            attn = attention_layers[i]
            flat = attn.view(B, -1)
            k = int(flat.size(1) * discard_ratio)
            threshold = torch.kthvalue(flat, k, dim=1, keepdim=True)[0]
            attention_layers[i] = torch.where(
                attn >= threshold.view(B, 1, 1),
                attn,
                torch.zeros_like(attn)
            )
            # Renormalize
            attention_layers[i] = attention_layers[i] / (attention_layers[i].sum(dim=-1, keepdim=True) + 1e-12)

    # Initialize with identity
    rollout = torch.eye(N, device=device).unsqueeze(0).expand(B, -1, -1)

    # Accumulate through layers
    for attn in attention_layers:
        if include_residual:
            # Account for residual: x_l = x_{l-1} + Attn(x_{l-1})
            # Effective attention: 0.5 * attn + 0.5 * identity
            rollout = attn @ (0.5 * rollout + 0.5 * torch.eye(N, device=device).unsqueeze(0))
        else:
            rollout = attn @ rollout

    return rollout


# ============================================================================
# DEPENDENCY DISTANCE
# ============================================================================

def compute_dependency_distance(
    attention: torch.Tensor,
    grid_size: int,
    normalize: bool = True,
    exclude_cls: bool = True
) -> Dict[str, torch.Tensor]:
    """
    Measure spatial distance of attention dependencies.

    Quantifies long-range reasoning:
    - High distance = attention spans far regions
    - Low distance = local focus

    Args:
        attention: [B, num_heads, N, N] where N = num_patches + 1
        grid_size: Patches per side (14 for 224px / 16px patches)
        normalize: Divide by image diagonal for scale invariance
        exclude_cls: Remove CLS token from analysis

    Returns:
        {
            'mean': average distance,
            'std': std across heads,
            'per_head': [num_heads] array
        }
    """
    B, H, N, _ = attention.shape

    # Create patch coordinates (CLS excluded)
    if exclude_cls:
        num_patches = N - 1
        coords = np.array([[i, j] for i in range(grid_size)
                          for j in range(grid_size)], dtype=np.float32)
    else:
        # Include CLS at (0, 0) arbitrarily
        num_patches = N
        coords = np.concatenate([[[0, 0]],
                                [[i, j] for i in range(grid_size)
                                 for j in range(grid_size)]], axis=0)

    # Pairwise distances
    dist_matrix = cdist(coords, coords, metric='euclidean')

    if normalize:
        diagonal = np.sqrt(2) * grid_size
        dist_matrix = dist_matrix / diagonal

    dist_tensor = torch.from_numpy(dist_matrix).float().to(attention.device)

    # Extract patch-to-patch attention
    if exclude_cls:
        attn_patches = attention[:, :, 1:, 1:]  # [B, H, num_patches, num_patches]
    else:
        attn_patches = attention

    # Weight distances by attention
    weighted_dist = attn_patches * dist_tensor.unsqueeze(0).unsqueeze(0)

    # Average over key dimension (sum because attention sums to 1)
    avg_dist_per_query = weighted_dist.sum(dim=-1)  # [B, H, num_patches]

    # Average over queries and batch
    per_head = avg_dist_per_query.mean(dim=(0, 2))  # [H]
    mean_dist = per_head.mean()
    std_dist = per_head.std()

    return {
        'mean': mean_dist.item(),
        'std': std_dist.item(),
        'per_head': per_head.cpu().numpy()
    }


# ============================================================================
# SPATIAL COHERENCE (MORAN'S I)
# ============================================================================

def compute_spatial_coherence(
    attention: torch.Tensor,
    grid_size: int,
    exclude_cls: bool = True
) -> Dict[str, float]:
    """
    Measure spatial autocorrelation of attention using Moran's I.

    Interpretation:
    - I > 0: Clustered attention (patches attend to neighbors)
    - I ≈ 0: Random spatial pattern
    - I < 0: Dispersed attention (anti-correlated)

    This measures whether attention forms coherent spatial regions (objects)
    or is scattered across the image.

    Args:
        attention: [B, num_heads, N, N]
        grid_size: Patches per side
        exclude_cls: Exclude CLS token

    Returns:
        {
            'moran_mean': Mean Moran's I across heads,
            'moran_std': Std across heads
        }

    Complexity: O(B * H * P^2) where P = num_patches
    """
    B, H, N, _ = attention.shape

    if exclude_cls:
        attn_patches = attention[:, :, 1:, 1:]
        num_patches = N - 1
    else:
        attn_patches = attention
        num_patches = N

    # Build spatial weight matrix (4-connectivity)
    W = _build_spatial_weights(grid_size).to(attention.device)
    W_sum = W.sum()

    # Compute Moran's I per head
    morans_per_head = []

    for h in range(H):
        attn_head = attn_patches[:, h]  # [B, num_patches, num_patches]

        # Average attention per query patch: [B, num_patches]
        attn_values = attn_head.mean(dim=-1)

        # Compute Moran's I for each batch
        batch_morans = []
        for b in range(B):
            x = attn_values[b]
            x_mean = x.mean()
            x_centered = x - x_mean

            # Numerator: spatial covariance
            numerator = (W * torch.outer(x_centered, x_centered)).sum()

            # Denominator: variance
            denominator = (x_centered ** 2).sum()

            if denominator > 1e-8:
                I = (num_patches / W_sum) * (numerator / denominator)
            else:
                I = torch.tensor(0.0, device=attention.device)

            batch_morans.append(I)

        morans_per_head.append(torch.stack(batch_morans).mean())

    morans_tensor = torch.stack(morans_per_head)  # [H]

    return {
        'moran_mean': morans_tensor.mean().item(),
        'moran_std': morans_tensor.std().item()
    }


def _build_spatial_weights(grid_size: int) -> torch.Tensor:
    """
    Build 4-connectivity spatial weight matrix.

    W[i, j] = 1 if patches i and j share an edge, else 0.

    Complexity: O(grid_size^2) construction, reusable.
    """
    num_patches = grid_size * grid_size
    W = torch.zeros(num_patches, num_patches)

    for i in range(grid_size):
        for j in range(grid_size):
            idx = i * grid_size + j

            # Right
            if j < grid_size - 1:
                W[idx, i * grid_size + (j + 1)] = 1
            # Bottom
            if i < grid_size - 1:
                W[idx, (i + 1) * grid_size + j] = 1
            # Left
            if j > 0:
                W[idx, i * grid_size + (j - 1)] = 1
            # Top
            if i > 0:
                W[idx, (i - 1) * grid_size + j] = 1

    return W


# ============================================================================
# ATTENTION ENTROPY
# ============================================================================

def compute_attention_entropy(
    attention: torch.Tensor,
    exclude_cls: bool = True
) -> Dict[str, float]:
    """
    Compute Shannon entropy of attention distributions.

    Measures focus:
    - Low entropy = attention concentrates on few patches
    - High entropy = attention diffuse across many patches

    Fine-tuning hypothesis: Should reduce entropy (sharpen attention).

    Args:
        attention: [B, num_heads, N, N]
        exclude_cls: Exclude CLS token

    Returns:
        {
            'entropy_mean': Mean entropy across heads,
            'entropy_std': Std across heads
        }
    """
    B, H, N, _ = attention.shape

    if exclude_cls:
        attn_patches = attention[:, :, 1:, 1:]
    else:
        attn_patches = attention

    # Compute Shannon entropy per row (query)
    # H = -sum(p * log(p))
    eps = 1e-12
    log_attn = torch.log(attn_patches + eps)
    entropy = -(attn_patches * log_attn).sum(dim=-1)  # [B, H, num_patches]

    # Average over queries and batch
    entropy_per_head = entropy.mean(dim=(0, 2))  # [H]

    return {
        'entropy_mean': entropy_per_head.mean().item(),
        'entropy_std': entropy_per_head.std().item()
    }


# ============================================================================
# LAYER-WISE COMPARISON
# ============================================================================

def compare_attention_layerwise(
    attention_baseline: List[torch.Tensor],
    attention_finetuned: List[torch.Tensor],
    grid_size: int
) -> Dict[str, Dict[str, List[float]]]:
    """
    Compare attention structure layer-by-layer.

    Core hypothesis test: Does fine-tuning reshape attention?

    Args:
        attention_baseline: List of [B, H, N, N] from pretrained model
        attention_finetuned: List of [B, H, N, N] from fine-tuned model
        grid_size: Patches per side

    Returns:
        {
            'baseline': {
                'distance': [L],
                'coherence': [L],
                'entropy': [L]
            },
            'finetuned': { ... }
        }
    """
    num_layers = len(attention_baseline)

    results = {
        'baseline': {'distance': [], 'coherence': [], 'entropy': []},
        'finetuned': {'distance': [], 'coherence': [], 'entropy': []}
    }

    for layer_idx in range(num_layers):
        # Baseline
        attn_b = attention_baseline[layer_idx]
        dist_b = compute_dependency_distance(attn_b, grid_size)
        coh_b = compute_spatial_coherence(attn_b, grid_size)
        ent_b = compute_attention_entropy(attn_b)

        results['baseline']['distance'].append(dist_b['mean'])
        results['baseline']['coherence'].append(coh_b['moran_mean'])
        results['baseline']['entropy'].append(ent_b['entropy_mean'])

        # Fine-tuned
        attn_f = attention_finetuned[layer_idx]
        dist_f = compute_dependency_distance(attn_f, grid_size)
        coh_f = compute_spatial_coherence(attn_f, grid_size)
        ent_f = compute_attention_entropy(attn_f)

        results['finetuned']['distance'].append(dist_f['mean'])
        results['finetuned']['coherence'].append(coh_f['moran_mean'])
        results['finetuned']['entropy'].append(ent_f['entropy_mean'])

    return results


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def extract_cls_attention(attention: torch.Tensor) -> torch.Tensor:
    """
    Extract CLS token attention to patches.

    Args:
        attention: [B, num_heads, N, N]

    Returns:
        cls_attn: [B, num_heads, N-1] - CLS attending to patches only
    """
    return attention[:, :, 0, 1:]


def extract_patch_attention(attention: torch.Tensor) -> torch.Tensor:
    """
    Extract patch-to-patch attention (exclude CLS).

    Args:
        attention: [B, num_heads, N, N]

    Returns:
        patch_attn: [B, num_heads, N-1, N-1]
    """
    return attention[:, :, 1:, 1:]


def statistical_significance(baseline: List[float], finetuned: List[float]) -> Dict[str, float]:
    """
    Paired t-test for layer-wise metrics.

    Args:
        baseline: Metric values per layer (pretrained)
        finetuned: Metric values per layer (fine-tuned)

    Returns:
        {'t_stat': float, 'p_value': float}
    """
    from scipy.stats import ttest_rel

    t_stat, p_value = ttest_rel(baseline, finetuned)

    return {
        't_stat': float(t_stat),
        'p_value': float(p_value)
    }