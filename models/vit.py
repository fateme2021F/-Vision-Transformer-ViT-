# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AVda_iwJTNZdOQYAcQvIPjAXOF21gjSh
"""

import torch
import torch.nn as nn
import timm
from typing import List, Dict, Optional
from functools import partial


class ViTWithAttention(nn.Module):
    """
    ViT wrapper with bug-free attention extraction.

    Key design decisions:
    - Factory functions prevent closure bugs
    - Attention stored per-layer with explicit indices
    - Original forward methods backed up for safe restoration
    """

    def __init__(self, config, pretrained=True):
        super().__init__()

        self.config = config

        # Load ViT backbone
        self.backbone = timm.create_model(
            config.MODEL_NAME,
            pretrained=pretrained,
            num_classes=0
        )

        # Classification head
        self.classifier = nn.Sequential(
            nn.LayerNorm(self.backbone.embed_dim),
            nn.Dropout(config.DROPOUT),
            nn.Linear(self.backbone.embed_dim, config.NUM_CLASSES)
        )

        nn.init.trunc_normal_(self.classifier[2].weight, std=0.02)
        nn.init.zeros_(self.classifier[2].bias)

        # Architecture info
        self.num_layers = len(self.backbone.blocks)
        self.num_heads = self.backbone.blocks[0].attn.num_heads
        self.embed_dim = self.backbone.embed_dim
        self.num_patches = self.backbone.patch_embed.num_patches

        # Attention storage: explicit per-layer indexing
        self._attention_maps: List[Optional[torch.Tensor]] = [None] * self.num_layers
        self._original_forwards: Dict[int, callable] = {}
        self._hooks_active = False

        print(f"✓ ViT: {self.num_layers} layers, {self.num_heads} heads, "
              f"{self.num_patches} patches")

    def _create_attention_hook(self, layer_idx: int):
        """
        Factory function to create attention capture hook.

        CRITICAL: Uses partial to bind layer_idx, preventing closure bugs.
        """
        def hook(attn_module, x):
            """
            Hook that captures attention weights.

            Inserted after softmax, before dropout in ViT attention:
                attn = (q @ k.T) * scale
                attn = softmax(attn)     # <-- CAPTURE HERE
                attn = dropout(attn)
            """
            B, N, C = x.shape

            # Standard ViT attention computation
            qkv = attn_module.qkv(x).reshape(B, N, 3, attn_module.num_heads,
                                              C // attn_module.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv.unbind(0)

            attn = (q @ k.transpose(-2, -1)) * attn_module.scale
            attn = attn.softmax(dim=-1)

            # Store attention for THIS layer (explicit indexing prevents overwrites)
            # Shape: [B, num_heads, N, N] where N = num_patches + 1 (includes CLS)
            self._attention_maps[layer_idx] = attn.detach().cpu()

            # Continue forward
            attn = attn_module.attn_drop(attn)
            x = (attn @ v).transpose(1, 2).reshape(B, N, C)
            x = attn_module.proj(x)
            x = attn_module.proj_drop(x)

            return x

        return hook

    def enable_attention_hooks(self):
        """
        Replace attention forward methods with hooks.

        Thread-safe: stores originals, uses factory functions.
        """
        if self._hooks_active:
            return

        for layer_idx, block in enumerate(self.backbone.blocks):
            attn_module = block.attn

            # Backup original
            self._original_forwards[layer_idx] = attn_module.forward

            # Create hook for THIS layer (factory prevents closure bug)
            hook = self._create_attention_hook(layer_idx)

            # Replace forward
            attn_module.forward = partial(hook, attn_module)

        self._hooks_active = True

    def disable_attention_hooks(self):
        """Restore original forward methods."""
        if not self._hooks_active:
            return

        for layer_idx, block in enumerate(self.backbone.blocks):
            if layer_idx in self._original_forwards:
                block.attn.forward = self._original_forwards[layer_idx]

        self._original_forwards.clear()
        self._attention_maps = [None] * self.num_layers
        self._hooks_active = False

    def get_attention_maps(self, x: torch.Tensor) -> List[torch.Tensor]:
        """
        Extract attention maps for input x.

        Args:
            x: [B, 3, H, W]

        Returns:
            List of attention tensors, one per layer.
            Each tensor: [B, num_heads, num_tokens, num_tokens]
            where num_tokens = num_patches + 1 (includes CLS)

            Index interpretation:
            - attention[:, :, 0, :] = CLS token attention
            - attention[:, :, 1:, 1:] = patch-to-patch attention
        """
        # Enable hooks
        was_training = self.training
        self.eval()
        self.enable_attention_hooks()

        # Clear previous
        self._attention_maps = [None] * self.num_layers

        # Forward pass
        with torch.no_grad():
            _ = self.backbone(x)

        # Copy results (defensive)
        attention_maps = [attn.clone() if attn is not None else None
                         for attn in self._attention_maps]

        # Cleanup
        self.disable_attention_hooks()
        if was_training:
            self.train()

        # Verify no Nones
        assert all(a is not None for a in attention_maps), \
            "Some layers did not capture attention"

        return attention_maps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Standard forward pass."""
        features = self.backbone(x)
        logits = self.classifier(features)
        return logits

    def freeze_backbone(self):
        """Freeze for baseline experiment."""
        for param in self.backbone.parameters():
            param.requires_grad = False

    def unfreeze_all(self):
        """Unfreeze for fine-tuning."""
        for param in self.parameters():
            param.requires_grad = True

    def get_parameter_groups(self):
        """Layer-wise learning rates."""
        no_decay = ['bias', 'norm', 'ln', 'bn']

        backbone_decay = []
        backbone_no_decay = []
        head_decay = []
        head_no_decay = []

        for name, param in self.backbone.named_parameters():
            if not param.requires_grad:
                continue
            if any(nd in name.lower() for nd in no_decay):
                backbone_no_decay.append(param)
            else:
                backbone_decay.append(param)

        for name, param in self.classifier.named_parameters():
            if not param.requires_grad:
                continue
            if any(nd in name.lower() for nd in no_decay):
                head_no_decay.append(param)
            else:
                head_decay.append(param)

        return [
            {'params': backbone_decay, 'lr': self.config.BACKBONE_LR,
             'weight_decay': self.config.WEIGHT_DECAY},
            {'params': backbone_no_decay, 'lr': self.config.BACKBONE_LR,
             'weight_decay': 0.0},
            {'params': head_decay, 'lr': self.config.HEAD_LR,
             'weight_decay': self.config.WEIGHT_DECAY},
            {'params': head_no_decay, 'lr': self.config.HEAD_LR,
             'weight_decay': 0.0}
        ]

    def __del__(self):
        """Cleanup on destruction."""
        self.disable_attention_hooks()


def test_attention_extraction():
    """Verify correctness."""
    import sys
    sys.path.append('..')
    from config import Config

    config = Config()
    model = ViTWithAttention(config, pretrained=False)

    x = torch.randn(2, 3, 224, 224)
    attention_maps = model.get_attention_maps(x)

    print(f"\n=== Attention Extraction Test ===")
    print(f"Input: {x.shape}")
    print(f"Captured {len(attention_maps)} layers")

    for i, attn in enumerate(attention_maps):
        print(f"Layer {i}: {attn.shape}")
        assert attn.shape == (2, model.num_heads, model.num_patches + 1, model.num_patches + 1)
        assert torch.allclose(attn.sum(dim=-1), torch.ones_like(attn.sum(dim=-1)), atol=1e-5)
        assert (attn >= 0).all()

    print("✓ All tests passed")


if __name__ == "__main__":
    test_attention_extraction()