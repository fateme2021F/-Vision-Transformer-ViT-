# Output Directory Documentation

This directory contains checkpoints, extracted attention data, computed metrics, and visualizations produced by the training and analysis pipeline. All files are generated by scripts in `experiments/` and `analysis/` modules.

## Directory Structure

```
outputs/
├── checkpoints/
│   ├── baseline_best.pth
│   └── finetuned_best.pth
├── attention_tensors/          (optional, not saved by default)
│   ├── baseline_layer_{0-11}.pt
│   └── finetuned_layer_{0-11}.pt
├── metrics/
│   └── attention_comparison.json
├── visualizations/
│   └── gradcam_samples/
│       ├── {category}_{id}_baseline.png
│       └── {category}_{id}_finetuned.png
└── summary.md
```

## Evaluated Models

Two models were trained and analyzed:

**Baseline Model** (`checkpoints/baseline_best.pth`):
- Pretrained ViT-B/16 backbone (frozen parameters)
- Classification head trained from random initialization
- Saved at epoch of best validation accuracy during training

**Fine-Tuned Model** (`checkpoints/finetuned_best.pth`):
- Pretrained ViT-B/16 backbone (all parameters trainable)
- Layer-wise learning rate decay applied
- Saved at epoch of best validation accuracy during training

Each checkpoint file (`.pth`) contains:
- `model_state_dict`: PyTorch model parameters
- `optimizer_state_dict`: Optimizer state at save time
- `scheduler_state_dict`: Learning rate scheduler state
- `epoch`: Training epoch number
- `best_val_acc`: Validation accuracy at save time
- `config`: Training configuration dictionary

## Computed Metrics

Three spatial metrics were computed per transformer layer (12 layers total) for both models:

1. **Dependency Distance**: Average spatial distance between attending patches, weighted by attention probability, normalized by image diagonal. CLS token excluded.

2. **Spatial Coherence (Moran's I)**: Spatial autocorrelation statistic computed over attention values using 4-connectivity spatial weights. CLS token excluded.

3. **Attention Entropy**: Shannon entropy of attention distributions over key patches. CLS token excluded.

All metrics are:
- Computed from attention weights extracted after softmax, before dropout
- Averaged across attention heads (12 heads per layer)
- Averaged across test set samples (number of samples processed determined by `--max_batches` parameter)
- Stored as floating-point values per layer

## Output File Formats

### `metrics/attention_comparison.json`

JSON file containing layer-wise metric values for both models.

**Structure**:
```json
{
  "baseline": {
    "distance": [float, float, ..., float],
    "coherence": [float, float, ..., float],
    "entropy": [float, float, ..., float]
  },
  "finetuned": {
    "distance": [float, float, ..., float],
    "coherence": [float, float, ..., float],
    "entropy": [float, float, ..., float]
  }
}
```

**Array indexing**: Each array contains exactly 12 elements, indexed 0-11, corresponding to transformer layers 0-11 (layer 0 = first transformer block, layer 11 = final transformer block).

**Example format** (values are illustrative):
```json
{
  "baseline": {
    "distance": [0.387, 0.392, 0.398, ..., 0.448],
    "coherence": [0.143, 0.151, 0.158, ..., 0.215],
    "entropy": [2.678, 2.692, 2.705, ..., 2.818]
  },
  "finetuned": {
    "distance": [0.368, 0.371, 0.374, ..., 0.329],
    "coherence": [0.167, 0.176, 0.184, ..., 0.263],
    "entropy": [2.543, 2.537, 2.529, ..., 2.349]
  }
}
```

To access the dependency distance for the 5th transformer block (index 4) of the fine-tuned model:
```python
import json
with open('outputs/metrics/attention_comparison.json', 'r') as f:
    data = json.load(f)
distance_layer4 = data['finetuned']['distance'][4]
```

### `visualizations/gradcam_samples/`

Directory containing Grad-CAM heatmap images for selected test samples.

**File naming convention**: `{category}_{image_id}_{model}.png`
- `category`: Scene category from MIT Indoor-67 (e.g., `kitchen`, `bedroom`, `office`)
- `image_id`: Unique identifier for the test image
- `model`: Either `baseline` or `finetuned`

**Image format**: PNG, RGB colormap applied to normalized gradient magnitudes, upsampled to original image resolution (224×224 pixels).

**Grad-CAM computation**: Applied to the final transformer block (layer 11) using gradients with respect to predicted class logits.

Each test image has two corresponding visualizations (one per model) for direct comparison.

### `attention_tensors/` (Optional)

If attention tensors are saved to disk (controlled by `SAVE_ATTENTION` flag in config), they are stored as PyTorch tensor files.

**File naming**: `{model}_layer_{layer_index}.pt`
- `model`: `baseline` or `finetuned`
- `layer_index`: 0-11

**Tensor shape**: `[num_samples, num_heads, num_tokens, num_tokens]`
- `num_samples`: Number of test images processed (batch_size × max_batches)
- `num_heads`: 12 (fixed for ViT-B/16)
- `num_tokens`: 197 (196 spatial patches + 1 CLS token)

**Loading example**:
```python
import torch
attn = torch.load('outputs/attention_tensors/finetuned_layer_5.pt')
# attn.shape = [num_samples, 12, 197, 197]
# attn[i, j, k, l] = attention weight from token k to token l, head j, sample i
```

Attention matrices are probability distributions: each row sums to 1.0 (within floating-point precision).

### `summary.md`

Markdown document containing structured interpretation of metric results, statistical significance tests, and methodological notes. Generated by analysis scripts. Contains textual descriptions, no raw data.

## Reading Layer-Wise Results

All metrics are computed per layer. To compare baseline vs. fine-tuned:

1. Load `attention_comparison.json`
2. Extract arrays for a given metric (e.g., `distance`)
3. Compare element-wise: `baseline['distance'][i]` vs `finetuned['distance'][i]` for layer `i`

Statistical tests (if performed) compare the 12-element arrays using paired tests across layers.

**Layer progression**: Lower indices (0-5) correspond to earlier processing stages; higher indices (6-11) correspond to later stages. Layer 11 is the final transformer block before the classification head.

**Metric units**:
- Dependency distance: Normalized by image diagonal (unitless, range approximately [0, 1])
- Spatial coherence: Moran's I statistic (unitless, typical range approximately [-1, 1])
- Attention entropy: Shannon entropy in bits (range [0, log₂(num_patches)])

## Reproducibility and Variability

**Deterministic components**:
- Attention extraction (given fixed model weights)
- Metric computation from attention tensors
- Layer-wise array indexing

**Potential sources of variation**:
- Training: Classification accuracy may vary slightly across hardware due to non-deterministic CUDA operations
- Random sampling: If test set is subsampled, different samples may be selected across runs
- Floating-point precision: Metrics may differ in the last decimal places across platforms

**Fixed elements**:
- Model architecture (ViT-B/16)
- Number of layers (12)
- Number of heads per layer (12)
- Patch grid size (14×14)
- CLS token position (index 0)

**To reproduce exactly**:
- Use identical random seed
- Process the same number of test batches (`--max_batches` parameter)
- Use the same checkpoint files
- Use deterministic CUDA mode (if available)

## Limitations on Interpretation

This documentation describes file contents and formats only. It does not:

- Explain why metric values differ between models
- Claim that any observed pattern is meaningful
- Recommend actions based on metric values
- Assert causality or correlation
- Evaluate metric quality or validity
- Interpret Grad-CAM visualizations

For interpretation guidance, see `summary.md` in this directory and the main repository README.

## File Validation

To verify output completeness:

**Required files**:
- `checkpoints/baseline_best.pth` (exists)
- `checkpoints/finetuned_best.pth` (exists)
- `metrics/attention_comparison.json` (exists, valid JSON)

**Expected properties**:
- JSON contains keys: `baseline`, `finetuned`
- Each model contains keys: `distance`, `coherence`, `entropy`
- Each metric array has length 12
- All values are finite floating-point numbers

**Validation script**:
```python
import json
from pathlib import Path

# Check files exist
assert Path('outputs/checkpoints/baseline_best.pth').exists()
assert Path('outputs/checkpoints/finetuned_best.pth').exists()
assert Path('outputs/metrics/attention_comparison.json').exists()

# Validate JSON structure
with open('outputs/metrics/attention_comparison.json', 'r') as f:
    data = json.load(f)

for model in ['baseline', 'finetuned']:
    assert model in data
    for metric in ['distance', 'coherence', 'entropy']:
        assert metric in data[model]
        assert len(data[model][metric]) == 12
        assert all(isinstance(v, (int, float)) for v in data[model][metric])

print("Output validation: PASSED")
```

## Contact

For questions about file formats or missing outputs: [Technical contact]

For questions about metric computation: See `analysis/attention.py` source code

For questions about interpretation: See `summary.md` and main repository README