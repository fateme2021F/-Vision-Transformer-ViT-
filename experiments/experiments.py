# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AVda_iwJTNZdOQYAcQvIPjAXOF21gjSh
"""

"""
experiments/compare.py - Layer-wise Attention Comparison

Runnable script for hypothesis testing.
No visualization dependencies in core logic.

Usage:
    python experiments/compare.py \
        --baseline outputs/baseline_best.pth \
        --finetuned outputs/finetuned_best.pth
"""

import torch
import argparse
import json
from pathlib import Path
from typing import List
from tqdm import tqdm

import sys
sys.path.append('..')

from models.vit import ViTWithAttention
from analysis.attention import (
    compare_attention_layerwise,
    statistical_significance
)


def load_model(checkpoint_path: Path, config, device) -> ViTWithAttention:
    """Load model from checkpoint."""
    model = ViTWithAttention(config, pretrained=True)
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()

    print(f"✓ Loaded: {checkpoint_path.name}")
    print(f"  Epoch: {checkpoint.get('epoch', '?')}")
    print(f"  Accuracy: {checkpoint.get('best_val_acc', 0)*100:.2f}%")

    return model


def extract_attention_from_loader(
    model: ViTWithAttention,
    dataloader,
    device,
    max_batches: int = 50
) -> List[torch.Tensor]:
    """
    Extract attention from dataset.

    Returns:
        List of [B_total, num_heads, N, N], one per layer
    """
    num_layers = model.num_layers
    attention_per_layer = [[] for _ in range(num_layers)]

    with torch.no_grad():
        for batch_idx, (images, _) in enumerate(tqdm(dataloader, desc="Extracting")):
            if batch_idx >= max_batches:
                break

            images = images.to(device)
            attention_maps = model.get_attention_maps(images)

            # Store each layer
            for layer_idx, attn in enumerate(attention_maps):
                attention_per_layer[layer_idx].append(attn)

    # Concatenate batches per layer
    attention_stacked = [torch.cat(layer_list, dim=0) for layer_list in attention_per_layer]

    print(f"✓ Extracted {len(attention_stacked)} layers, "
          f"{attention_stacked[0].size(0)} samples")

    return attention_stacked


def print_results(results: dict):
    """Print comparison results."""
    print("\n" + "="*70)
    print("ATTENTION STRUCTURE COMPARISON")
    print("="*70)

    # Distance
    baseline_dist = results['baseline']['distance']
    finetuned_dist = results['finetuned']['distance']
    print(f"\nDependency Distance (normalized):")
    print(f"  Baseline:   {sum(baseline_dist)/len(baseline_dist):.4f}")
    print(f"  Fine-tuned: {sum(finetuned_dist)/len(finetuned_dist):.4f}")
    print(f"  Difference: {sum(baseline_dist)/len(baseline_dist) - sum(finetuned_dist)/len(finetuned_dist):.4f}")

    sig_dist = statistical_significance(baseline_dist, finetuned_dist)
    p_str = "***" if sig_dist['p_value'] < 0.001 else "**" if sig_dist['p_value'] < 0.01 else "*" if sig_dist['p_value'] < 0.05 else "ns"
    print(f"  p-value: {sig_dist['p_value']:.4f} {p_str}")

    # Coherence
    baseline_coh = results['baseline']['coherence']
    finetuned_coh = results['finetuned']['coherence']
    print(f"\nSpatial Coherence (Moran's I):")
    print(f"  Baseline:   {sum(baseline_coh)/len(baseline_coh):.4f}")
    print(f"  Fine-tuned: {sum(finetuned_coh)/len(finetuned_coh):.4f}")
    print(f"  Difference: {sum(finetuned_coh)/len(finetuned_coh) - sum(baseline_coh)/len(baseline_coh):.4f}")

    sig_coh = statistical_significance(baseline_coh, finetuned_coh)
    p_str = "***" if sig_coh['p_value'] < 0.001 else "**" if sig_coh['p_value'] < 0.01 else "*" if sig_coh['p_value'] < 0.05 else "ns"
    print(f"  p-value: {sig_coh['p_value']:.4f} {p_str}")

    # Entropy
    baseline_ent = results['baseline']['entropy']
    finetuned_ent = results['finetuned']['entropy']
    print(f"\nAttention Entropy (bits):")
    print(f"  Baseline:   {sum(baseline_ent)/len(baseline_ent):.4f}")
    print(f"  Fine-tuned: {sum(finetuned_ent)/len(finetuned_ent):.4f}")
    print(f"  Difference: {sum(baseline_ent)/len(baseline_ent) - sum(finetuned_ent)/len(finetuned_ent):.4f}")

    sig_ent = statistical_significance(baseline_ent, finetuned_ent)
    p_str = "***" if sig_ent['p_value'] < 0.001 else "**" if sig_ent['p_value'] < 0.01 else "*" if sig_ent['p_value'] < 0.05 else "ns"
    print(f"  p-value: {sig_ent['p_value']:.4f} {p_str}")

    print("="*70)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--baseline', type=Path, required=True)
    parser.add_argument('--finetuned', type=Path, required=True)
    parser.add_argument('--output', type=Path, default=Path('outputs'))
    parser.add_argument('--max_batches', type=int, default=50)
    args = parser.parse_args()

    # Config
    from config import Config
    config = Config()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}\n")

    # Load data
    from data.dataset import create_dataloaders
    _, _, test_loader, _ = create_dataloaders(config)

    # Load models
    print("Loading models...")
    model_baseline = load_model(args.baseline, config, device)
    model_finetuned = load_model(args.finetuned, config, device)

    # Extract attention
    print("\nExtracting baseline attention...")
    attn_baseline = extract_attention_from_loader(
        model_baseline, test_loader, device, args.max_batches
    )

    print("\nExtracting fine-tuned attention...")
    attn_finetuned = extract_attention_from_loader(
        model_finetuned, test_loader, device, args.max_batches
    )

    # Compare
    print("\nComputing layer-wise metrics...")
    grid_size = config.IMAGE_SIZE // 16  # ViT-B/16

    results = compare_attention_layerwise(
        attn_baseline,
        attn_finetuned,
        grid_size
    )

    # Print
    print_results(results)

    # Save
    args.output.mkdir(parents=True, exist_ok=True)
    output_path = args.output / 'attention_comparison.json'

    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\n✓ Results saved: {output_path}")


if __name__ == "__main__":
    main()